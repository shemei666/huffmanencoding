{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e7881e-ed77-47ef-8b66-99b5516adb0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Abstract\n",
    "\n",
    "**Huffman Encoding** is one of the most basic and elegant applications of the **Greedy Algorithm Design Paradigm**. It provides an optimal method of **lossless data compression** by assigning shorter binary codes to frequently occurring symbols and longer codes to rarely occurring ones.\n",
    "This project implements Huffman Encoding and Decoding in **Python 3.14.0**, constructs frequency tables and Huffman Trees, and evaluates performance through compression ratio analysis.  \n",
    "The project presents both the **proof of correctness** and **complexity analysis** of the algorithm and compares the bit cost with that of **fixed-length encoding**.\n",
    "Experimental results show that Huffman encoding achieves **significant space reduction** when symbol distributions are **non-uniform**, validating its theoretical optimality in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2eee01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Introduction and Motivation\n",
    "\n",
    "In the digital era, where vast quantities of data are produced every second, efficient storage and transmission have become crucial. **Data compression** seeks to represent information using fewer bits than the original form, thereby saving both storage space and communication bandwidth.\n",
    "\n",
    "Among several compression methods, **Huffman Encoding** stands out as a classic example of a **Greedy Algorithm** that achieves **lossless compression**. It assigns variable-length binary codes to symbols such that frequently occurring symbols receive shorter codes, while rare symbols are given longer codes. But this needs to be done carefully as it is susceptible to ambiguous interpretation.To emphasize this we consider the example of the Morse code,\n",
    "\n",
    "\n",
    "Traditionally, according to the number of unique characters, we assign a fixed-length bit string to represent each symbol, as done in encoding schemes such as ASCII. However, characters in real-world data do not occur uniformly—some appear far more frequently than others. To save space, the characters that occur more often should be assigned shorter bit strings, while those that occur less frequently can be assigned longer ones. The challenge, therefore, is to determine an optimal way of assigning these variable-length codes so as to minimize the overall number of bits required.\n",
    "\n",
    "Morse code, in a way, follows a similar intuition by assigning simpler symbols or shorter actions to frequently occurring letters. However, it does not satisfy the **prefix-free property**; instead, it relies on fixed time intervals to mark the beginning and end of each letter.\n",
    "\n",
    "**Huffman Encoding** effectively addresses this limitation by constructing a prefix-free variable-length code that minimizes the total expected code length, thereby achieving optimal lossless compression.\n",
    "\n",
    "---\n",
    "\n",
    "## **Objectives**\n",
    "The goals of this project are to:\n",
    "1. Implement Huffman Encoding and Decoding in Python.  \n",
    "2. Construct frequency tables and Huffman Trees for textual data.  \n",
    "3. Compute and analyze **compression ratios** against fixed-length encoding.  \n",
    "4. Verify the **prefix-free property** and correctness of the algorithm.  \n",
    "5. Analyze time and space complexities and demonstrate optimality through experiment.\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Compression and Information Theory**\n",
    "**Data compression** can be broadly classified as:\n",
    "- **Lossless Compression:** The original data is perfectly reconstructed after decompression (e.g., Huffman Coding).  \n",
    "- **Lossy Compression:** Some information is irreversibly discarded to achieve higher compression (e.g., JPEG, MP3).\n",
    "\n",
    "We focus on the Huffman algorithm, which minimizes the expected number of bits per symbol. The theoretical foundation behind the **Huffman Algorithm (1952)** lies in **Claude Shannon’s Information Theory (1948)**, where the entropy of a source, given by:\n",
    "\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n",
    "$$\n",
    "\n",
    "represents the lower bound on the average number of bits required to encode a message. Huffman’s algorithm constructs a code whose average length \\(L\\) satisfies:\n",
    "\n",
    "$$\n",
    "H(X) \\leq L < H(X) + 1\n",
    "$$\n",
    "\n",
    "showing that it is asymptotically optimal among all prefix-free codes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Why Huffman Algorithm is a Greedy Algorithm**\n",
    "\n",
    "The **Huffman Algorithm** is a *greedy algorithm* as it satisfies both the **greedy-choice property** and the **optimal substructure property**.  \n",
    "It works by making the best decision at each step and then proceeding to the next, again choosing the best possible option at that moment — that is, it makes **locally optimal choices** at every stage.\n",
    "\n",
    "- **Greedy-choice property:** A global optimum can be achieved by choosing a local optimum at each step.  \n",
    "- **Optimal substructure:** An optimal solution to the problem contains optimal solutions to its subproblems.\n",
    "\n",
    "Together, these ensure that Huffman’s locally optimal merges of the smallest frequencies lead to a globally optimal prefix-free code.\n",
    "\n",
    "\n",
    "## **Applications**\n",
    "Huffman Encoding (or, more broadly, the idea behind it) has numerous applications:\n",
    "- **File Compression:** Used in ZIP, GZIP, and DEFLATE formats for efficient lossless compression.  \n",
    "- **Image Compression:** Serves as the entropy coding stage in JPEG.  \n",
    "- **Audio Compression:** Applied in MP3 and AAC to encode frequency coefficients efficiently.  \n",
    "- **Data Transmission:** Reduces bandwidth usage in communication systems and embedded devices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96039d56",
   "metadata": {},
   "source": [
    "# **2. Problem Definition and Objectives**\n",
    "\n",
    "---\n",
    "\n",
    "# **Problem Statement**\n",
    "\n",
    "---\n",
    "\n",
    "## **Formalized Description**\n",
    "\n",
    "**Input:**\n",
    "\n",
    "- Alphabet \\( A = (a_1, a_2, \\dots, a_n) \\), which is the symbol alphabet of size \\( n \\).  \n",
    "- Tuple \\( W = (w_1, w_2, \\dots, w_n) \\), which is the tuple of positive symbol weights (usually proportional to probabilities), i.e.  \n",
    "  \\( w_i = \\text{weight}(a_i), \\quad i \\in \\{1, 2, \\dots, n\\} \\).\n",
    "\n",
    "**Output:**\n",
    "\n",
    "- Code $ C(W) = (c_1, c_2, \\dots, c_n) $, which is the tuple of binary codewords, where \\( c_i \\) is the codeword assigned to \\( a_i \\), \n",
    "  for \\( i \\in \\{1, 2, \\dots, n\\} \\).\n",
    "\n",
    "**Goal:**\n",
    "\n",
    "Let  \n",
    "\n",
    "\\[\n",
    "L(C(W)) = \\sum_{i=1}^{n} w_i \\times \\text{length}(c_i)\n",
    "\\]\n",
    "\n",
    "be the weighted path length of code \\( C \\).  \n",
    "The objective is to minimize \\( L(C(W)) \\), satisfying the condition  \n",
    "\n",
    "\\[\n",
    "L(C(W)) \\leq L(T(W))\n",
    "\\]\n",
    "\n",
    "for any other code \\( T(W) \\).\n",
    "\n",
    "---\n",
    "\n",
    "## **Informal Description**\n",
    "\n",
    "**Given:**  \n",
    "A set of symbols \\( S \\), and for each symbol \\( x \\in S \\), the frequency \\( f_x \\) representing the fraction of symbols in the text that are equal to \\( x \\).\n",
    "\n",
    "**Find:**  \n",
    "A prefix-free binary code (a set of codewords) with minimum expected codeword length — equivalently, a binary tree with minimum weighted path length from the root.\n",
    "\n",
    "---\n",
    "\n",
    "*Source: Adapted and summarized from the [“Huffman coding” article on Wikipedia](https://en.wikipedia.org/wiki/Huffman_coding).*\n",
    "\n",
    "\n",
    "## **Input and Output Specification**\n",
    "\n",
    "- **Input:** A text file or string containing characters.  \n",
    "- **Output:**\n",
    "  1. Frequency table and Huffman tree visualization.  \n",
    "  2. Encoded binary sequence.  \n",
    "  3. Decoded text identical to input.  \n",
    "  4. Compression statistics (ratio, space saved).\n",
    "\n",
    "---\n",
    "\n",
    "## **Assumptions and Constraints**\n",
    "\n",
    "- Each input character has a non-negative frequency.  \n",
    "- All symbols are independent and identically distributed within the input.  \n",
    "- Encoding must remain prefix-free for correctness.\n",
    "\n",
    "---\n",
    "\n",
    "## **Design Goals**\n",
    "\n",
    "1. Maintain correctness and unique decodability.  \n",
    "2. Ensure minimal average codeword length.  \n",
    "3. Optimize both runtime and memory usage.  \n",
    "4. Compare results with fixed-length encoding using \\( \\lceil \\log_2 n \\rceil \\) bits per character.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccaded",
   "metadata": {},
   "source": [
    "# **3. Algorithm Design**\n",
    "\n",
    "## **Algorithmic Overview**\n",
    "\n",
    "The Huffman algorithm is **greedy** in nature — it always tries to obtain the most optimal outcome at each step without considering future consequences.  \n",
    "It does so by constructing a **prefix-free binary tree**, built iteratively by combining the two least frequent nodes into a single new node and assigning the two original nodes as its left and right children.\n",
    "\n",
    "In other words, a queue (or priority list) of nodes is maintained, from which the two nodes with the smallest frequencies are repeatedly removed, merged into a new combined node, and then reinserted into the queue. This process continues until only one node remains, which becomes the root of the Huffman tree.\n",
    "\n",
    "The algorithm can be divided into the following stages:\n",
    "1. Build a **frequency table** for all unique characters in the text.  \n",
    "2. Create a **min-heap (priority queue)** of nodes sorted by frequency.  \n",
    "3. Repeatedly remove the two nodes with the smallest frequencies and merge them into a new node whose frequency is their sum.  \n",
    "4. Insert the new node back into the heap until only one node remains — the root of the Huffman Tree.  \n",
    "5. Traverse the tree to assign binary codes: left edge adds a `0` and right edge adds a `1`.\n",
    "\n",
    "---\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Structures Used**\n",
    "- **Priority Queue (Min-Heap):** Ensures efficient extraction of nodes with smallest frequencies, implemented via Python’s `heapq`.  \n",
    "- **Binary Tree:** Represents hierarchical structure of merged nodes.  \n",
    "- **Dictionary:** Stores mapping from characters to their Huffman codes.  \n",
    "\n",
    "---\n",
    "## **Pseudocode**\n",
    "\n",
    "The following pseudocode describes the construction of the Huffman Tree and the generation of the Huffman Codes.\n",
    "\n",
    "---\n",
    "\n",
    "### **Algorithm 1: Build Huffman Tree**\n",
    "\n",
    "Input: Set of symbols C = {c1, c2, ..., cn} with corresponding frequencies f(c)\n",
    "Output: Root node of the Huffman Tree\n",
    "```\n",
    "1. Create a min-heap Q and insert all characters with their frequencies.\n",
    "2. while size(Q) > 1 do\n",
    "3. x ← Extract-Min(Q)          // Node with smallest frequency\n",
    "4. y ← Extract-Min(Q)          // Node with second smallest frequency\n",
    "5. z ← New node with frequency f(z) = f(x) + f(y)\n",
    "6. z.left ← x\n",
    "7. z.right ← y\n",
    "8. Insert(Q, z)\n",
    "9. end while\n",
    "10. return Extract-Min(Q) // The remaining node is the root of the Huffman Tree\n",
    " ```\n",
    "---\n",
    "\n",
    "### **Algorithm 2: Generate Codes**\n",
    "\n",
    "Input: Root node of Huffman Tree, current code = \"\"\n",
    "Output: Code dictionary for each symbol\n",
    "```\n",
    "1. if node is leaf then\n",
    "2. Assign current code to symbol(node)\n",
    "3. else\n",
    "4. GenerateCodes(node.left, code + \"0\")\n",
    "5. GenerateCodes(node.right, code + \"1\")\n",
    "6. end if\n",
    "```\n",
    "---\n",
    "\n",
    "### **Explanation**\n",
    "\n",
    "- The **Build Huffman Tree** algorithm constructs the binary tree by repeatedly combining the two least frequent nodes into a new parent node, whose frequency equals their sum.  \n",
    "- The **Generate Codes** algorithm traverses the final tree recursively to assign binary codes:\n",
    "  - A left edge adds a `0`\n",
    "  - A right edge adds a `1`\n",
    "- The resulting codes are **prefix-free**, meaning no codeword is a prefix of another — ensuring unique and unambiguous decoding.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## **Example: \"abracadabra\"**\n",
    "\n",
    "Consider the input string `\"abracadabra\"`. The frequencies of the characters are:\n",
    "\n",
    "| Character | Frequency |\n",
    "|:----------:|:----------:|\n",
    "| a | 5 |\n",
    "| b | 2 |\n",
    "| r | 2 |\n",
    "| c | 1 |\n",
    "| d | 1 |\n",
    "\n",
    "The Huffman algorithm proceeds as follows:\n",
    "- Merge **c(1)** and **d(1)** → new node with frequency **2**  \n",
    "- Merge **b(2)** and **r(2)** → new node with frequency **4**  \n",
    "- Merge node(2) [from c,d] with node(4) [from b,r] → new node with frequency **6**  \n",
    "- Merge **a(5)** and node(6) → root node with frequency **11**\n",
    "\n",
    "**Resulting Huffman Codes:**\n",
    "\n",
    "| Character | Huffman Code |\n",
    "|:----------:|:-------------:|\n",
    "| a | 0 |\n",
    "| r | 100 |\n",
    "| b | 101 |\n",
    "| c | 110 |\n",
    "| d | 111 |\n",
    "\n",
    "---\n",
    "\n",
    "## **Encoded Output Length**\n",
    "\n",
    "The length of the Huffman code for the string `\"abracadabra\"` is: 01011000110011101011000  which consists of **23 bits** in total.  \n",
    "\n",
    "In contrast, the fixed-length encoding version would require:\n",
    "\n",
    "$$\n",
    "(\\text{length of \"abracadabra\"}) \\times \\lceil \\log_2(5) \\rceil = 11 \\times 3 = 33\n",
    "$$\n",
    "\n",
    "Hence, the fixed-length encoded version would be **33 bits**, while the Huffman-encoded version uses **23 bits**, giving a savings of **10 bits (~30.3% reduction)**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95211c3e",
   "metadata": {},
   "source": [
    "# **4. Correctness**\n",
    "\n",
    "---\n",
    "\n",
    "## **Key Definitions**\n",
    "\n",
    "**Definition (Prefix-Free Code):**  \n",
    "A set of binary codewords is prefix-free if no codeword is a prefix of another.\n",
    "\n",
    "**Definition (Expected Code Length):**  \n",
    "For probabilities $p_i$ and code lengths $\\ell_i$:\n",
    "$$\n",
    "L(C) = \\sum_i p_i \\ell_i\n",
    "$$\n",
    "The goal is to minimize $L(C)$.\n",
    "\n",
    "**Definition (Full Binary Tree Representation):**  \n",
    "Each prefix-free code corresponds to a full binary tree whose leaves represent the symbols.\n",
    "\n",
    "---\n",
    "\n",
    "## **Why the Greedy Step Is Valid**\n",
    "\n",
    "**Lemma (Sibling Property):**  \n",
    "In an optimal prefix-free code, the two least probable symbols are siblings at the greatest depth.\n",
    "\n",
    "**Proof:**  \n",
    "If not, suppose two other nodes $x', y'$ are deeper. Swapping their positions with the smallest two ($x, y$) can only reduce or preserve the total expected length.  \n",
    "Thus, we can always arrange for the smallest probabilities to appear as siblings.  \n",
    "\n",
    "This justifies merging the two smallest weights first in each step.\n",
    "\n",
    "---\n",
    "\n",
    "## **Recursive Optimality**\n",
    "\n",
    "**Theorem (Optimal Substructure):**  \n",
    "After merging $x$ and $y$ (the smallest nodes) into $z$, finding the optimal code for the reduced set and then expanding $z$ back gives the optimal code for the full set.\n",
    "\n",
    "**Proof:**  \n",
    "Let the merged probability be $p' = p_x + p_y$.  \n",
    "If $L'$ is the minimal length for the smaller instance, expanding $p'$ back adds one bit to each of $x$ and $y$, giving:\n",
    "$$\n",
    "L = L' + (p_x + p_y)\n",
    "$$\n",
    "Any better tree would imply a smaller $L'$, contradicting optimality.\n",
    "\n",
    "---\n",
    "\n",
    "## **Proof of Correctness**\n",
    "\n",
    "**Theorem (Correctness of Huffman Coding):**  \n",
    "Huffman’s algorithm produces a prefix-free code of minimum expected length.\n",
    "\n",
    "**Proof (By Induction on $n$):**  \n",
    "**Base Case:** For $n = 2$, only one valid prefix-free code exists.  \n",
    "**Inductive Step:** Assume the result holds for $n-1$ symbols.  \n",
    "For $n$ symbols, merge the smallest two into one symbol of weight $p'$.  \n",
    "By induction, the reduced tree is optimal; expanding $p'$ back preserves optimality.\n",
    "\n",
    "---\n",
    "\n",
    "## **Intuition Behind Huffman Encoding**\n",
    "\n",
    "The idea behind Huffman encoding is to assign probabilities to the occurrences of each character in the text and then define $L$ as the total length of the encoded string.  \n",
    "Ideally, we want to use as little space as possible to represent data. Hence, the algorithm attempts to minimize the **expected value of the code length**, i.e., the average number of bits required to represent a character.\n",
    "\n",
    "In other words, the algorithm aims to construct a variable-length binary code that minimizes the expected length $E[L]$ of the encoded message.  \n",
    "To achieve this, characters that occur more frequently are placed **closer to the root** of the Huffman tree (thus having shorter codes), while less frequent characters are placed **deeper in the tree** (thus having longer codes).\n",
    "\n",
    "This strategy ensures that, on average, the total number of bits used to represent a message is minimized.\n",
    "\n",
    "---\n",
    "\n",
    "## **Information-Theoretic Perspective**\n",
    "\n",
    "From an algorithmic point of view, Huffman encoding minimizes the expected code length $E[L]$ — the average number of bits required to represent a symbol based on its probability of occurrence.  \n",
    "However, from an **information-theoretic perspective**, the problem can be viewed through the lens of **Shannon’s Entropy**.\n",
    "\n",
    "Entropy, introduced by Claude Shannon in 1948, measures the fundamental limit of compressibility of any information source. It is defined as:\n",
    "$$\n",
    "H(X) = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n",
    "$$\n",
    "where $p_i$ denotes the probability of the $i^{th}$ symbol.\n",
    "\n",
    "Entropy represents the theoretical lower bound on the average number of bits needed to encode one symbol of the source.  \n",
    "In other words, no lossless compression algorithm can achieve an average code length smaller than $H(X)$.\n",
    "\n",
    "Huffman encoding, though derived from a greedy algorithmic process, turns out to be nearly optimal in the information-theoretic sense.  \n",
    "Its average code length $L$ satisfies the inequality:\n",
    "$$\n",
    "H(X) \\leq L < H(X) + 1\n",
    "$$\n",
    "This means that Huffman coding operates within one bit of the optimal limit imposed by entropy.\n",
    "\n",
    "Thus, while the algorithm was originally formulated from a procedural optimization viewpoint — minimizing expected code length — it naturally aligns with the deeper theoretical framework of information theory, showing that the greedy construction achieves almost the same efficiency as the best possible code predicted by Shannon’s theory.\n",
    "\n",
    "---\n",
    "\n",
    "In practical applications, when probabilities are estimated accurately, Huffman encoding approaches the entropy bound closely.  \n",
    "When probabilities vary with time or are unknown in advance, extensions such as **adaptive Huffman coding** and **arithmetic coding** are used to maintain or surpass this efficiency.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db389cf",
   "metadata": {},
   "source": [
    "# **5. Complexity Analysis**\n",
    "\n",
    "When we implemented the algorithm, we wanted to go beyond just quoting  \n",
    "the $O(n \\log n)$ result from class — to see how that complexity actually arises  \n",
    "and how the constant factors matter in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## **Counting the Work**\n",
    "\n",
    "Let $n$ be the number of distinct symbols.  \n",
    "The Huffman process performs $n-1$ merges.  \n",
    "Each merge removes two smallest elements and inserts one combined node.  \n",
    "If we store nodes in a **min-heap**, then:\n",
    "\n",
    "- **extract-min:** $O(\\log n)$  \n",
    "- **insert:** $O(\\log n)$\n",
    "\n",
    "Since each merge performs two removals and one insertion,  \n",
    "the total work is proportional to $3(n-1)\\log n$, i.e.\n",
    "\n",
    "$$\n",
    "T(n) = O(n \\log n)\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## **Why This Bound Makes Sense**\n",
    "\n",
    "The $\\log n$ factor comes from maintaining heap order.  \n",
    "If we used an unsorted list, each merge would require scanning all elements,  \n",
    "costing $O(n)$ per step — an $O(n^2)$ algorithm overall.  \n",
    "The heap avoids this by keeping the smallest elements near the top.\n",
    "\n",
    "---\n",
    "\n",
    "## **A Quicker Variant (If Already Sorted)**\n",
    "\n",
    "When input frequencies are sorted, we can build the same Huffman tree  \n",
    "in linear time using two queues — one for original symbols, one for merged nodes.  \n",
    "Each node is enqueued and dequeued once, giving $O(n)$ total time.  \n",
    "But for unsorted data, $O(n \\log n)$ is asymptotically optimal.\n",
    "\n",
    "---\n",
    "\n",
    "## **Amortized Reasoning**\n",
    "\n",
    "Every node enters and exits the heap exactly once.  \n",
    "Dividing the total work evenly, each node costs about $O(\\log n)$  \n",
    "time on average.  \n",
    "This shows the “amortized” perspective — average cost per operation,  \n",
    "not worst case per step.\n",
    "\n",
    "---\n",
    "\n",
    "## **A More Concrete Cost Picture**\n",
    "\n",
    "To visualize the cost, each merge step involves two `extract-min`  \n",
    "and one `insert`, each taking about $\\log n$ time.  \n",
    "Hence over $n-1$ merges,\n",
    "\n",
    "$$\n",
    "T(n) \\approx 3(n-1)\\log n + O(n)\n",
    "$$\n",
    "\n",
    "This explains why $O(n \\log n)$ is tight and why the heap is ideal here.\n",
    "\n",
    "---\n",
    "\n",
    "## **Encoding and Decoding**\n",
    "\n",
    "Once the tree is ready:\n",
    "\n",
    "- **Encoding:** $O(M)$ time, where $M$ is message length — we simply look up codes and append bits.  \n",
    "- **Decoding:** also $O(M)$, traversing one bit at a time from root to leaves.\n",
    "\n",
    "Both phases are practically linear in message size.\n",
    "\n",
    "---\n",
    "\n",
    "## **Space Complexity**\n",
    "\n",
    "Required data structures:\n",
    "\n",
    "- Heap (≤ $n$ elements)  \n",
    "- Huffman tree ($2n-1$ nodes)  \n",
    "- Code lookup table (1 entry per symbol)\n",
    "\n",
    "Hence $O(n)$ total space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Complexity Summary**\n",
    "\n",
    "| **Phase**           | **Time**         | **Space** |\n",
    "|---------------------|------------------|-----------|\n",
    "| Tree construction   | $O(n \\log n)$    | $O(n)$    |\n",
    "| Encoding            | $O(M)$           | $O(n)$    |\n",
    "| Decoding            | $O(M)$           | $O(n)$    |\n",
    "\n",
    "---\n",
    "\n",
    "In practice, runtime increased roughly as $n \\log n$, matching theory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddeb59e",
   "metadata": {},
   "source": [
    "# **6. Implementation and Experimental Results**\n",
    "\n",
    "---\n",
    "\n",
    "## **Programming Environment**\n",
    "This project was implemented in **Python 3.14.0** using standard library modules only.  \n",
    "The following libraries were used:\n",
    "- `heapq` — for implementing the min-heap (priority queue) used in tree construction.  \n",
    "- `collections.Counter` — for counting symbol frequencies.  \n",
    "- `math` — for logarithmic calculations in compression metrics.  \n",
    "- `os` and `sys` — for basic file handling and program control.\n",
    "\n",
    "All code was written and tested in a **Jupyter Notebook** and a standard **Python environment** (VS Code).  \n",
    "The implementation focuses on readability and simplicity, with detailed comments explaining every step of the Huffman Encoding and Decoding process.\n",
    "\n",
    "---\n",
    "\n",
    "## **Input and Output Interface**\n",
    "The program allows two ways to provide input:\n",
    "1. **Manual Text Input:**  \n",
    "   The user can enter a string directly into the console or notebook cell.\n",
    "2. **File Input:**  \n",
    "   A text file can be read from the system for encoding.\n",
    "\n",
    "The output includes:\n",
    "- A **frequency table** of characters.  \n",
    "- A **Huffman tree** constructed from those frequencies.  \n",
    "- The **encoded bitstring** (saved as `encoded_output.txt`).  \n",
    "- The **decoded text** (saved as `decoded_output.txt`).  \n",
    "- Compression statistics such as total bits, fixed-length bits, and percentage of space saved.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880eb78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **7. Experiments, Datasets and Observations**\n",
    "\n",
    "---\n",
    "\n",
    "## **Experimental Datasets**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Sample Run**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Compression Analysis**\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Verification**\n",
    "The decoded output perfectly matched the original input in every test, confirming that the algorithm works correctly.  \n",
    "The amount of space saved depended on how uneven the character frequencies were — when all characters appeared with similar frequency, compression was minimal, but when certain characters appeared much more often than others, the algorithm achieved a significant reduction in size.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9e7d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **8. Potential Issues**\n",
    "\n",
    "Although the Huffman Encoding implementation works well for moderate inputs, a few refinements can make it more robust.\n",
    "\n",
    "---\n",
    "\n",
    "## **Memory Management and Large File Handling**\n",
    "\n",
    "The current implementation loads the entire file into memory before computing frequencies and building the tree.  \n",
    "For large datasets, this is inefficient and may exceed memory limits.\n",
    "\n",
    "**Improvement:** Use a *streaming approach* — read data in small chunks and update frequency counts incrementally.  \n",
    "This allows handling larger files efficiently without exhausting memory.\n",
    "\n",
    "---\n",
    "\n",
    "## ** Binary Output Format: Bits vs ASCII Representation**\n",
    "\n",
    "Encoded data are currently stored as ASCII characters (`'0'` and `'1'`), wasting space since each bit uses an entire byte.\n",
    "\n",
    "**Improvement:** Output should be written as a true *bitstream*.  \n",
    "Python libraries such as `bitarray` or `io.BytesIO` can pack bits into bytes efficiently.  \n",
    "During decoding, bytes can be unpacked bit by bit for traversal, ensuring that compression gains accurately reflect theoretical efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ** Visualization and Interpretability**\n",
    "\n",
    "ASCII trees are easy to inspect for small examples but become unreadable for larger datasets.\n",
    "\n",
    "**Improvement:** Use visualization tools such as `Graphviz` or `NetworkX` to render trees graphically.  \n",
    "Color-coding leaves (symbols) and internal nodes (merged frequencies) would make the construction process more interpretable and pedagogically useful.\n",
    "\n",
    "---\n",
    "\n",
    "## ** Error Handling and Robustness**\n",
    "\n",
    "The current decoder assumes perfect input; a single corrupted bit may cause complete decoding failure.\n",
    "\n",
    "**Improvement:** Introduce input validation and fault tolerance.  \n",
    "Adding checksums or structured exception handling can help detect corrupted files and prevent runtime errors, making the implementation more reliable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf080f5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **9. Challenges Faced and Conclusion**\n",
    "\n",
    "---\n",
    "\n",
    "## **Challenges Faced**\n",
    "\n",
    "1. One of the main challenges we faced was setting up the programming environment. Installing Python, configuring the right libraries, and getting Jupyter and Git Bash to work properly took much longer than expected.\n",
    "\n",
    "2. Even though we understood the Huffman algorithm conceptually, turning it into working code was far from straightforward. It took time to make the computer do exactly what we wanted — especially while handling text files, file paths, and encoding issues. Debugging small mistakes like queue ordering or missing cases often took hours.\n",
    "\n",
    "<!-- 3. What surprised us most was how much effort it takes to correctly implement something that seems so basic on paper. Huffman’s algorithm is a simple algorithm, but even small modifications, misunderstandings, or blind spots make it tricky to get right in practice. -->\n",
    "\n",
    "In the end, the experience made us appreciate how theory and implementation are two very different skills — and how much precision and patience real-world coding actually requires.\n",
    "\n",
    "---\n",
    "\n",
    "## **Improvements**\n",
    "\n",
    "Generalizations and future enhancements include:\n",
    "\n",
    "- Implementing binary-level compression to measure true disk savings.  \n",
    "- Developing adaptive Huffman and arithmetic coding variants.  \n",
    "- Applying Huffman coding to multimedia (images, audio).  \n",
    "- Creating a graphical interface to visualize the encoding process.  \n",
    "- Integrating entropy-based analysis to compare performance with the Shannon limit.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "This project successfully implemented and analyzed **Huffman Encoding**, demonstrating its role as a classic example of a **greedy optimization algorithm**.  \n",
    "The results verified that Huffman’s approach minimizes the expected number of bits per symbol while preserving exact reconstructability.  \n",
    "\n",
    "The algorithm was found to be:\n",
    "\n",
    "- **Correct:** Decoding perfectly reconstructs the original text.  \n",
    "- **Efficient:** Exhibits near-linear runtime for practical datasets.  \n",
    "- **Optimal:** Achieves minimal weighted path length among all prefix-free codes.  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f34c85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Appendix**\n",
    "\n",
    "---\n",
    "\n",
    "## **A. Complete Python Code (Final Submission Version)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **B. How to Use the Code**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
