{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1e7881e-ed77-47ef-8b66-99b5516adb0b",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Abstract\n",
    "\n",
    "**Huffman Encoding** is one of the most basic and elegant applications of the **Greedy Algorithm Design Paradigm**. It provides an optimal method of **lossless data compression** by assigning shorter binary codes to frequently occurring symbols and longer codes to rarely occurring ones.\n",
    "This project implements Huffman Encoding and Decoding in **Python 3.14.0**, constructs frequency tables and Huffman Trees, and evaluates performance through compression ratio analysis.  \n",
    "The project presents both the **proof of correctness** and **complexity analysis** of the algorithm and compares the bit cost with that of **fixed-length encoding**.\n",
    "Experimental results show that Huffman encoding achieves **significant space reduction** when symbol distributions are **non-uniform**, validating its theoretical optimality in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2eee01",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# 1. Introduction and Motivation\n",
    "\n",
    "In the digital era, where vast quantities of data are produced every second, efficient storage and transmission have become crucial. **Data compression** seeks to represent information using fewer bits than the original form, thereby saving both storage space and communication bandwidth.\n",
    "\n",
    "Traditionally, according to the number of unique characters, we assign a fixed-length bit string to represent each symbol, as done in encoding schemes such as ASCII. However, characters in real-world data do not occur uniformly—some appear far more frequently than others. To save space, the characters that occur more often should be assigned shorter bit strings, while those that occur less frequently can be assigned longer ones. The challenge, therefore, is to determine an optimal way of assigning these variable-length codes to minimize the overall number of bits required.\n",
    "\n",
    "Among several compression methods, **Huffman Encoding** stands out as a classic example of a **Greedy Algorithm** that achieves **lossless compression**. It assigns variable-length binary codes to symbols such that frequently occurring symbols receive shorter codes, while rare symbols are given longer codes. But this needs to be done carefully as it is susceptible to ambiguous interpretation. To emphasize this we consider the example of the Morse code,\n",
    "![morse code](image_1.png)\n",
    "\n",
    "The Morse code was designed so that the duration to send each letter was inverse to the frequency of the letter occurring in usual English text. This is generally a good strategy, but we notice that in the encoding of the Morse code we see that a **J** could be interpreted as **AM**, this is not considered a problem in the Morse code since the letters are usually sent with a short time gap. However using this is inefficient for storage to use a separator. Hence, we need a code that cannot be interpreted in multiple ways while having variable length. So the property we need here is\n",
    "\n",
    "**Prefix free code:** is code system such that there is no whole code word in the system that is a prefix (initial segment) of any other code word in the system.\n",
    "\n",
    "\n",
    "\n",
    "Morse code, in a way, follows a similar intuition by assigning simpler symbols or shorter actions to frequently occurring letters. However, it does not satisfy the **prefix-free property**; instead, it relies on fixed time intervals to mark the beginning and end of each letter.\n",
    "\n",
    "**Huffman Encoding** effectively addresses this limitation by constructing a prefix-free variable-length code that minimizes the total expected code length, thereby achieving optimal lossless compression.\n",
    "\n",
    "---\n",
    "\n",
    "## Objectives\n",
    "The goals of this project are to:\n",
    "1. Implement Huffman Encoding and Decoding in Python.  \n",
    "2. Construct frequency tables and Huffman Trees for textual data.  \n",
    "3. Compute and analyze **compression ratios** against fixed-length encoding.  \n",
    "4. Verify the **prefix-free property** and correctness of the algorithm.  \n",
    "5. Analyze time and space complexities and demonstrate optimality through experiment.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **Why Huffman Algorithm is a Greedy Algorithm**\n",
    "\n",
    "The **Huffman Algorithm** is a *greedy algorithm* as it satisfies both the **greedy-choice property** and the **optimal substructure property**.  \n",
    "It works by making the best decision at each step and then proceeding to the next, again choosing the best possible option at that moment — that is, it makes **locally optimal choices** at every stage.\n",
    "\n",
    "- **Greedy-choice property:** A global optimum can be achieved by choosing a local optimum at each step.  \n",
    "- **Optimal substructure:** An optimal solution to the problem contains optimal solutions to its subproblems.\n",
    "\n",
    "Together, these ensure that Huffman’s locally optimal merges of the smallest frequencies lead to a globally optimal prefix-free code.\n",
    "\n",
    "\n",
    "## **Applications**\n",
    "Huffman Encoding (or, more broadly, the idea behind it) has numerous applications:\n",
    "- **File Compression:** Used in ZIP, GZIP, and DEFLATE formats for efficient lossless compression.  \n",
    "- **Image Compression:** Serves as the entropy coding stage in JPEG.  \n",
    "- **Audio Compression:** Applied in MP3 and AAC to encode frequency coefficients efficiently.  \n",
    "- **Data Transmission:** Reduces bandwidth usage in communication systems and embedded devices.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96039d56",
   "metadata": {},
   "source": [
    "# 2. Problem Definition and Objectives\n",
    "\n",
    "\n",
    "## Problem Statement\n",
    "\n",
    "\n",
    "### Formalized Description\n",
    "\n",
    "**Input:**\n",
    "\n",
    "1. Alphabet $A = (a_1, a_2, \\dots, a_n)$, which is the symbol alphabet of size $n$. \n",
    "2. Weights $W = (w_1, w_2, \\dots, w_n)$, which is the tuple of positive symbol weights (usually proportional to probabilities), i.e.\n",
    "$$\n",
    "w_i = \\text{weight}(a_i), \\quad i \\in \\mathbb{N}\n",
    "$$\n",
    "\n",
    "**Output:**\n",
    "\n",
    "Code $C(W) = (c_1, c_2, \\dots, c_n)$, which is the tuple of binary codewords, where $c_i$ is the codeword assigned to $a_i$,$\\forall i \\in \\mathbb{N}$ such that $C(W)$ has the properties:\n",
    "\n",
    "  1. $C(W)$ is a prefix-free code.\n",
    "  2. Let $L(C(W)) = \\sum_{i=1}^{n} w_i \\times \\text{length}(c_i)$ be the weighted path length of code $C$. $L(C(W)) \\leq L(T(W))$ for any other code $T$ satisfying the condition 1.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Informal Description\n",
    "\n",
    "**Given:**  \n",
    "A set of symbols $ A=(a_i)_{i=1}^n $, and for each symbol $ x \\in A $, the frequency $ w_i $ representing the fraction of symbols in the text that are equal to $a_i$.\n",
    "\n",
    "**Find:**  \n",
    "A prefix-free binary code (a set of codewords) with minimum expected codeword length.\n",
    "\n",
    "---\n",
    "\n",
    "*Source: Adapted and summarized from the [“Huffman coding” article on Wikipedia](https://en.wikipedia.org/wiki/Huffman_coding).*\n",
    "\n",
    "\n",
    "## Assumptions and Constraints\n",
    "\n",
    "- Each input character has a non-negative frequency.  \n",
    "- All symbols are independent and identically distributed within the input.  \n",
    "- The output encoding must be prefix-free for correctness.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fccaded",
   "metadata": {},
   "source": [
    "# 3. Algorithm Design\n",
    "\n",
    "## Algorithmic Overview\n",
    "\n",
    "The Huffman algorithm is **greedy** in nature — it always tries to obtain the most optimal outcome at each step without considering future consequences.  \n",
    "It does so by constructing a **prefix-free binary tree**, built iteratively by combining the two least frequent nodes into a single new node and assigning the two original nodes as its left and right children.\n",
    "\n",
    "In other words, a queue (or priority list) of nodes is maintained, from which the two nodes with the smallest frequencies are repeatedly removed, merged into a new combined node, and then reinserted into the queue. This process continues until only one node remains, which becomes the root of the Huffman tree.\n",
    "\n",
    "The algorithm can be divided into the following stages:\n",
    "1. Build a **frequency table** for all unique characters in the text.  \n",
    "2. Create a **min-heap (priority queue)** of nodes sorted by frequency.  \n",
    "3. Repeatedly remove the two nodes with the smallest frequencies and merge them into a new node whose frequency is their sum.  \n",
    "4. Insert the new node back into the heap until only one node remains — the root of the Huffman Tree.  \n",
    "5. Traverse the tree to assign binary codes: left edge adds a `0` and right edge adds a `1`.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Data Structures Used\n",
    "- **Priority Queue (Min-Heap):** Ensures efficient extraction of nodes with smallest frequencies, implemented via Python’s `heapq`.  \n",
    "- **Binary Tree:** Represents hierarchical structure of merged nodes.  \n",
    "- **Dictionary:** Stores mapping from characters to their Huffman codes.  \n",
    "\n",
    "---\n",
    "\n",
    "## Pseudocode\n",
    "\n",
    "The following pseudocode describes the construction of the Huffman Tree and the generation of the Huffman Codes.\n",
    "\n",
    "### Algorithm 1: Build Huffman Tree\n",
    "\n",
    "Input: Set of symbols C = {c1, c2, ..., cn} with corresponding frequencies f(c)\n",
    "Output: Root node of the Huffman Tree\n",
    "```\n",
    "1. Create a min-heap Q and insert all characters with their frequencies.\n",
    "2. while size(Q) > 1 do\n",
    "3. x ← Extract-Min(Q)          // Node with smallest frequency\n",
    "4. y ← Extract-Min(Q)          // Node with second smallest frequency\n",
    "5. z ← New node with frequency f(z) = f(x) + f(y)\n",
    "6. z.left ← x\n",
    "7. z.right ← y\n",
    "8. Insert(Q, z)\n",
    "9. end while\n",
    "10. return Extract-Min(Q) // The remaining node is the root of the Huffman Tree\n",
    " ```\n",
    "---\n",
    "\n",
    "### Algorithm 2: Generate Codes\n",
    "\n",
    "Input: Root node of Huffman Tree, current code = \"\"\n",
    "Output: Code dictionary for each symbol\n",
    "```\n",
    "1. if node is leaf then\n",
    "2. Assign current code to symbol(node)\n",
    "3. else\n",
    "4. GenerateCodes(node.left, code + \"0\")\n",
    "5. GenerateCodes(node.right, code + \"1\")\n",
    "6. end if\n",
    "```\n",
    "---\n",
    "\n",
    "### Explanation\n",
    "\n",
    "- The **Build Huffman Tree** algorithm constructs the binary tree by repeatedly combining the two least frequent nodes into a new parent node, whose frequency equals their sum.  \n",
    "- The **Generate Codes** algorithm traverses the final tree recursively to assign binary codes:\n",
    "  - A left edge adds a `0`\n",
    "  - A right edge adds a `1`\n",
    "- The resulting codes are **prefix-free**, meaning no codeword is a prefix of another — ensuring unique and unambiguous decoding.\n",
    "\n",
    "\n",
    "**Example: \"abracadabra\"**\n",
    "\n",
    "Consider the input string `\"abracadabra\"`. The frequencies of the characters are:\n",
    "\n",
    "| Character | Frequency |\n",
    "|:----------:|:----------:|\n",
    "| a | 5 |\n",
    "| b | 2 |\n",
    "| r | 2 |\n",
    "| c | 1 |\n",
    "| d | 1 |\n",
    "\n",
    "The Huffman algorithm proceeds as follows:\n",
    "- Merge **c(1)** and **d(1)** → new node with frequency **2**  \n",
    "- Merge **b(2)** and **r(2)** → new node with frequency **4**  \n",
    "- Merge node(2) [from c,d] with node(4) [from b,r] → new node with frequency **6**  \n",
    "- Merge **a(5)** and node(6) → root node with frequency **11**\n",
    "\n",
    "**Resulting Huffman Codes:**\n",
    "\n",
    "| Character | Huffman Code |\n",
    "|:----------:|:-------------:|\n",
    "| a | 0 |\n",
    "| r | 100 |\n",
    "| b | 101 |\n",
    "| c | 110 |\n",
    "| d | 111 |\n",
    "\n",
    "---\n",
    "\n",
    "## **Encoded Output Length**\n",
    "\n",
    "The length of the Huffman code for the string `\"abracadabra\"` is: 01011000110011101011000  which consists of **23 bits** in total.  \n",
    "\n",
    "In contrast, the fixed-length encoding version would require:\n",
    "\n",
    "$$\n",
    "(\\text{length of \"abracadabra\"}) \\times \\lceil \\log_2(5) \\rceil = 11 \\times 3 = 33\n",
    "$$\n",
    "\n",
    "Hence, the fixed-length encoded version would be **33 bits**, while the Huffman-encoded version uses **23 bits**, giving a savings of **10 bits (~30.3% reduction)**.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95211c3e",
   "metadata": {},
   "source": [
    "# 4. Correctness\n",
    "\n",
    "### Key Definitions\n",
    "\n",
    "**Definition (Prefix-Free Code):** A set of binary codewords is prefix-free if no codeword is a prefix of another.\n",
    "\n",
    "**Definition (Expected Code Length):** For probabilities $p_i$ and code lengths $\\ell_i$:\n",
    "$$\n",
    "L(C) = \\sum_i p_i \\ell_i\n",
    "$$\n",
    "\n",
    "The goal is to minimize $L(C)$.\n",
    "\n",
    "**Definition (Full Binary Tree):** A binary tree such that each node has 2 children or is a leaf node. \n",
    "\n",
    "**Definition (Optimal Binary Tree):** An optimal binary tree is a $T$ of alphabet $A=(a_1,\\dots,a_n)$ and corresponding weights $W=(w_1,\\dots,w_n)$ is a full binary tree with its leaf nodes labelled with elements of $A$ and the quantity\n",
    "$$\n",
    "L(T) = \\sum_{a_i\\text{ leaf of } T} w_i \\times \\text{depth}(a_i)\n",
    "$$\n",
    "is minimized.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdf35516",
   "metadata": {},
   "source": [
    "\n",
    "## Proof of Correctness and Optimality\n",
    "\n",
    "**Lemma 4.1:** At the end of each step of Huffman's algorithm the heap contains either a labelled node or a binary tree \n",
    "**Proof:** We prove by induction on the iteration count.\n",
    "\n",
    "**Base Case:** On the first iteration we start with only labelled nodes in the heap, hence after we take the least frequent elements $L_1,L_2$ from the heap and create a full binary tree with an unlabeled root whose left child is $L_1$ and right child $L_2$.\n",
    "\n",
    "**Induction Step:** \n",
    "Assume that the statement holds after the $n-1$-th iteration. Then before running the $n$-th iteration all elements in the heap are either labelled or full binary trees. Now we take the least frequent elements $L_1,L_2$ then by induction hypothesis we have that $L_1,L_2$ are labelled node or binary tree, Now we create a node $R$ of which the left child is $L_1$ and right child is $L_2$, we just need to check that the node $R$ is the root node of a full binary tree, We see that $R$ has exactly 2 children. Now by induction hypothesis the child nodes could be labelled nodes or binary trees, hence after joining we get another binary tree.\n",
    "\n",
    "**Lemma 4.2:** Any code from a full binary tree with symbols as leaf nodes produce a prefix-free code.\n",
    "**Proof:** Trivial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc48ee7",
   "metadata": {},
   "source": [
    "\n",
    "**Lemma 4.3:** Let $T$ be a binary tree whose leaf nodes are labeled by $A$ and has the leaves have corresponding weights $W$ also let $x$ and $y$ be 2 leaves in $T$ with weights $w_x,w_y$. Then if $T'$ is a tree made by swapping $x$ and $y$\n",
    "$$\n",
    "L(T') - L(T) = (w_y - w_x) (\\text{depth}(x,T)-\\text{depth}(y,T))\n",
    "$$\n",
    "**Proof:**\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(T') - L(T) &= w_y \\text{depth}(x, T) + w_x \\text{depth}(y, T) - w_x \\text{depth}(w, T) - w_y \\text{depth}(y, T) \\\\\n",
    "&= w_y(\\text{depth}(x, T) - \\text{depth}(y, T)) + w_x(\\text{depth}(y, T) - \\text{depth}(x, T)) \\\\\n",
    "&= (w_y - w_x)(\\text{depth}(x, T) - \\text{depth}(y, T))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "**Lemma 4.4:** There exists an optimal binary tree such that symbols with the least weights are siblings\n",
    "\n",
    "**Proof:** Let $T$ be any optimal tree let $x,y$ be the symbols with leasts weights. By definition, they are always leaf nodes. If there are more than 2 symbols which have the same least frequency take the ones with most depth in the tree.\n",
    "\n",
    "If $x,y$ are already siblings there is nothing to do, otherwise we have two cases, we assume W.L.O.G $\\text{depth}(x) >= \\text{depth}(y)$\n",
    "**Case 1:** $x$ has a sibling leaf node $z$\n",
    "We create $T'$ by swapping $y$ and $z$ in $T$, then by **Lemma 4.3** we get $L(T') - L(T) = (w_y - w_z)(\\text{depth}(z)-\\text{depth}(y)) \\leq 0$, But $T$ is optimal hence $T'$ is also optimal.\n",
    "\n",
    "**Case 2:** $x$ does not have a sibling leaf node and hence there is a leaf node $z$ with depth greater than $\\text{depth}(x)$.\n",
    "We swap $x$ and $z$ to get $T'$ and by our choice that $x$,$y$ are the least weighted and having the most depth we have that $w_x < w_z$ and hence by **Lemma 4.3** we have $L(T')<L(T)$ contradicting minimality. So this case is not possible. $\\blacksquare$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef11c73",
   "metadata": {},
   "source": [
    "**Theorem (Correctness of Huffman Coding):** Huffman’s algorithm produces a prefix-free code of minimum expected length.\n",
    "\n",
    "**Proof (By Induction on alphabet size $|A|$):**\n",
    "Initial tree <=> code equivalence todo\n",
    "\n",
    "**Base Case:** For $|A| = 1$ the algorithm terminates without entering the loop, and it gives a tree with one node labelled $a_1$, this tree will has $L(T)=0$ hence already at minimum.\n",
    "\n",
    "**Inductive Step:** We have by the induction hypothesis that Huffman algorithm gives an optimal tree for alphabet of size $n-1$. Now we need to prove for alphabets of size $n$. If $A = (a_1,\\dots,a_{n-1},a_n)$ and $W=(w_1,\\dots,w_{n-1},w_n)$ we create a new alphabet $A' = (a_1,\\dots,a_{n-2},z)$ and $W' = (w_1,\\dots,w_{n-2},w_z =w_{n-1}+w_{n})$. After the first iteration of the algorithm `HuffmanTree(A,W)`, the 2 least frequent elements are made into children of a new node with frequency equal to their sum and hence it will run through the remaining iterations with the modified alphabet $A'$ and weight $W'$ hence the algorithm proceeds the same as `HuffmanTree(A',W')`. By our induction hypothesis we have that `HuffmanTree(A',W')` will give a optimal tree $T'$. Hence $T$ from `HuffmanTree(A,W)` is $T'$ with a binary tree containing $a_n,a_{n-1}$ as children in the place of $z$\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "L(T) & = \\sum_{a \\in A'} w_a \\text{depth}(a,T) + w_x \\text{depth}(x,T) + w_y \\text{depth}(y,T)\\\\\n",
    "& = \\sum_{a \\in A'} w_a \\text{depth}(a,T) + w_z\\left(\\text{depth}(z,T') + 1\\right) \\\\\n",
    "& = \\sum_{a \\in A'} w_a \\text{depth}(a,T') + w_z = \\sum w_a \\text{depth}(a,T') + w_x + w_y \\\\\n",
    "& = L(T') + w_x + w_y\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Now assume for contradiction that $T$ is not optimal. Let $S$ be an optimal tree of $A$ that contains $a_n,a_{n-1}$ as siblings by **Lemma 4.4**. Now by removing $a_{n},a_{n-1}$ from $Z$ and labelling their parent $z$ with weight as their sum we get another full binary tree $Z'$. We can repeat the same calculation above to get $L(Z) = L(Z') + w_x + w_y$. Hence we get $L(T') =L(T) - w_x -w_y > L(Z) - w_x - w_y = L(Z')$ which is a contradiction as $L(T')$ is optimal for $A$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db389cf",
   "metadata": {},
   "source": [
    "# 5. Complexity Analysis\n",
    "\n",
    "We now discuss the complexity of each of the algorithms involved in this process.\n",
    "1. Huffman Tree\n",
    "2. Encoding\n",
    "3. Decoding\n",
    "\n",
    "\n",
    "### Huffman tree\n",
    "We first need to initialize a heap using the weights of the alphabet, to heapify this it takes $O(\\log |A|)$ time. At each iteration of the loop 2 elements are popped from the heap and an element is pushed to the heap, hence the size of the heap decreases by 1. Hence the loop will run exactly $|A|-1$ many times.\n",
    "\n",
    "At the ith iteration of the loop, we have that the heap has length $|A|-i|$, hence the push and pop operations on the min-heap takes $O(\\log (|A|-i))$ time each. So the total time complexity will be \n",
    "$$\n",
    "\\sum_{i=1}^{|A|-1} 3O(\\log |A|-i) = O(|A|\\log |A|)\n",
    "$$\n",
    "\n",
    "## **Encoding and Decoding**\n",
    "\n",
    "Once the tree is ready:\n",
    "\n",
    "- **Encoding:** $O(M)$ time, where $M$ is message length — we simply look up codes and append bits.  \n",
    "- **Decoding:** also $O(M)$, traversing one bit at a time from root to leaves.\n",
    "\n",
    "Both phases are practically linear in message size.\n",
    "\n",
    "---\n",
    "\n",
    "## **Space Complexity**\n",
    "\n",
    "Required data structures:\n",
    "\n",
    "- Heap (≤ $n$ elements)  \n",
    "- Huffman tree ($2n-1$ nodes)  \n",
    "- Code lookup table (1 entry per symbol)\n",
    "\n",
    "Hence $O(n)$ total space.\n",
    "\n",
    "---\n",
    "\n",
    "## **Complexity Summary**\n",
    "\n",
    "| **Phase**           | **Time**         | **Space** |\n",
    "|---------------------|------------------|-----------|\n",
    "| Tree construction   | $O(n \\log n)$    | $O(n)$    |\n",
    "| Encoding            | $O(M)$           | $O(M)$    |\n",
    "| Decoding            | $O(M)$           | $O(M)$    |\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddeb59e",
   "metadata": {},
   "source": [
    "# **6. Implementation and Experimental Results**\n",
    "\n",
    "## **Programming Environment**\n",
    "This project was implemented in **Python 3.14.0** using standard library modules only.  \n",
    "The following libraries were used:\n",
    "- `heapq` — for implementing the min-heap (priority queue) used in tree construction.  \n",
    "- `math` — for logarithmic calculations in compression metrics.  \n",
    "- `os` and `sys` — for basic file handling and program control.\n",
    "\n",
    "All code was written and tested in a **Jupyter Notebook** and a standard **Python environment**\n",
    "The implementation focuses on readability and simplicity, with detailed comments explaining every step of the Huffman Encoding and Decoding process.\n",
    "\n",
    "---\n",
    "\n",
    "## **Input and Output Interface**\n",
    "The program allows two ways to provide input:\n",
    "1. **Manual Text Input:**  \n",
    "   The user can enter a string directly into the console or notebook cell.\n",
    "2. **File Input:**  \n",
    "   A text file can be read from the system for encoding.\n",
    "\n",
    "The output includes:\n",
    "- A **frequency table** of characters.  \n",
    "- A **Huffman tree** constructed from those frequencies.  \n",
    "- The **encoded bitstring** (saved as `encoded_output.txt`).  \n",
    "- The **decoded text** (saved as `decoded_output.txt`).  \n",
    "- Compression statistics such as total bits, fixed-length bits, and percentage of space saved.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf46bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "\n",
    "class Node:\n",
    "    def __init__(self, char, freq, left, right):\n",
    "        self.char = char\n",
    "        self.freq = freq\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "    def __lt__(self, o):\n",
    "        return self.freq < o.freq\n",
    "\n",
    "    def __str__(self):\n",
    "        if self.char:\n",
    "            return self.char\n",
    "        return f\"[{self.left.__str__()},{self.right.__str__()}]\"\n",
    "\n",
    "\n",
    "\n",
    "def get_frequency_heap(text):\n",
    "    freq_dict = {}\n",
    "    heap = []\n",
    "    for char in text:\n",
    "        if char in freq_dict:\n",
    "            freq_dict[char] += 1\n",
    "        else:\n",
    "            freq_dict[char] = 1\n",
    "    for char, freq in freq_dict.items():\n",
    "        node = Node(char, freq, None, None)\n",
    "        heapq.heappush(heap, node)\n",
    "    return heap\n",
    "\n",
    "\n",
    "def huffman_tree(text):\n",
    "    min_heap = get_frequency_heap(text)\n",
    "    if not min_heap:\n",
    "        return None\n",
    "    while len(min_heap) > 1:\n",
    "        l1 = heapq.heappop(min_heap)\n",
    "        l2 = heapq.heappop(min_heap)\n",
    "        new_node = Node(\"\", l1.freq + l2.freq, l1, l2)\n",
    "        heapq.heappush(min_heap, new_node)\n",
    "\n",
    "    return min_heap[0]\n",
    "\n",
    "\n",
    "def huffman_dict(node):\n",
    "    if node.char:\n",
    "        return {node.char: \"\"}\n",
    "    return {\n",
    "        **{char: \"0\" + code for char, code in huffman_dict(node.left).items()},\n",
    "        **{char: \"1\" + code for char, code in huffman_dict(node.right).items()},\n",
    "    }\n",
    "\n",
    "\n",
    "def huffman_encode(text, dict):\n",
    "    encoded = \"\"\n",
    "    for char in text:\n",
    "        encoded += dict[char]\n",
    "    return encoded\n",
    "\n",
    "\n",
    "def huffman_decode(code, dict):\n",
    "    invert_dict = {value: key for key, value in dict.items()}\n",
    "    decoded = \"\"\n",
    "    buffer = \"\"\n",
    "    for bit in code:\n",
    "        buffer += bit\n",
    "        if buffer in invert_dict:\n",
    "            decoded += invert_dict[buffer]\n",
    "            buffer = \"\"\n",
    "    return decoded\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f880eb78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **7. Experiments, Datasets and Observations**\n",
    "\n",
    "\n",
    "## **Experimental Datasets**\n",
    "\n",
    "| Dataset | Category | Size(bytes) |\n",
    "| :--- | :--- | :--- |\n",
    "| E.coli | Complete genome of the E. Coli bacterium | 4638690 |\n",
    "| bible | The King James version of the bible | 4047392 |\n",
    "| world | The CIA world fact book | 2473400 |\n",
    "\n",
    "<br>\n",
    "\n",
    "| Dataset | Category | Size |\n",
    "| :--- | :--- | :--- |\n",
    "| bib | Bibliography (refer format) | 111261 |\n",
    "| book1 | Fiction book | 768771 |\n",
    "| book2 | Non-fiction book (troff format) | 610856 |\n",
    "| geo | Geophysical data | 102400 |\n",
    "| news | USENET batch file | 377109 |\n",
    "| obj1 | Object code for VAX | 21504 |\n",
    "| obj2 | Object code for Apple Mac | 246814 |\n",
    "| paper1 | Technical paper | 53161 |\n",
    "| paper2 | Technical paper | 82199 |\n",
    "| pic | Black and white fax picture | 513216 |\n",
    "| progc | Source code in \"C\" | 39611 |\n",
    "| progl | Source code in LISP | 71646 |\n",
    "| progp | Source code in PASCAL | 49379 |\n",
    "| trans | Transcript of terminal session | 93695 |\n",
    "\n",
    "---\n",
    "\n",
    "## **Compression Analysis**\n",
    "\n",
    "Firstly we load our datasets into python\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f50f9423",
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus_files = [\n",
    "    'E.coli',\n",
    "    'bible.txt',\n",
    "    'world192.txt',\n",
    "]\n",
    "\n",
    "calgary_corpus_files = [\n",
    "    'bib', 'book1', 'book2', 'news',  \n",
    "    'paper1', 'paper2','paper3','paper4','paper5','paper6', 'progc', 'progl', 'progp' \n",
    "]\n",
    "\n",
    "LARGE_CORPUS_DIR = \"./large\"\n",
    "CALGARY_CORPUS_DIR = \"./calgary\"\n",
    "\n",
    "datasets = {}\n",
    "for file in large_corpus_files:\n",
    "    with open(f\"{LARGE_CORPUS_DIR}/{file}\",\"r\",encoding=\"ascii\") as f:\n",
    "        datasets[file] = f.read()\n",
    "\n",
    "# for file in calgary_corpus_files:\n",
    "#     with open(f\"{CALGARY_CORPUS_DIR}/{file}\",\"r\",encoding=\"ascii\") as f:\n",
    "#         datasets[file] = f.read()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0469bce2",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict_keys' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m huffmansize = []\n\u001b[32m      7\u001b[39m fixedlengthsize = []\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name,text \u001b[38;5;129;01min\u001b[39;00m \u001b[43mdatasets\u001b[49m\u001b[43m.\u001b[49m\u001b[43mitems\u001b[49m():\n\u001b[32m      9\u001b[39m     tree = huffman_tree(text)\n\u001b[32m     10\u001b[39m     code = huffman_dict(tree)\n",
      "\u001b[31mAttributeError\u001b[39m: 'dict_keys' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "asciisize = []\n",
    "huffmansize = []\n",
    "fixedlengthsize = []\n",
    "for name,text in datasets.items():\n",
    "    tree = huffman_tree(text)\n",
    "    code = huffman_dict(tree)\n",
    "    encoded = huffman_encode(text,code)\n",
    "    print(len(code))\n",
    "    fixedlengthsize += [math.ceil(math.log(len(code))) * len(text) // 1000]\n",
    "    asciisize += [(len(text)*8) // 1000]\n",
    "    huffmansize += [len(encoded)//8000]\n",
    "\n",
    "\n",
    "\n",
    "datasetnames = datasets.keys()\n",
    "datasize = {\n",
    "    'ASCII': asciisize,\n",
    "    'Fixed Length': fixedlengthsize,\n",
    "    'Huffman': huffmansize,\n",
    "}\n",
    "\n",
    "x = np.arange(len(datasetnames))  # the label locations\n",
    "width = 0.25  # the width of the bars\n",
    "multiplier = 0\n",
    "\n",
    "fig, ax = plt.subplots(layout='constrained')\n",
    "\n",
    "for attribute, measurement in datasize.items():\n",
    "    offset = width * multiplier\n",
    "    rects = ax.bar(x + offset, measurement, width, label=attribute)\n",
    "    ax.bar_label(rects, padding=3)\n",
    "    multiplier += 1\n",
    "\n",
    "# Add some text for labels, title and custom x-axis tick labels, etc.\n",
    "ax.set_ylabel('Size in KB')\n",
    "ax.set_title('Compression')\n",
    "ax.set_xticks(x + width, datasetnames)\n",
    "ax.legend(loc='upper right', ncols=3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57f27856",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **Verification**\n",
    "The decoded output perfectly matched the original input in every test, confirming that the algorithm works correctly.  \n",
    "The amount of space saved depended on how uneven the character frequencies were — when all characters appeared with similar frequency, compression was minimal, but when certain characters appeared much more often than others, the algorithm achieved a significant reduction in size.\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c9e7d5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **8. Potential Issues**\n",
    "\n",
    "Although the Huffman Encoding implementation works well for moderate inputs, a few refinements can make it more robust.\n",
    "\n",
    "---\n",
    "\n",
    "## **Memory Management and Large File Handling**\n",
    "\n",
    "The current implementation loads the entire file into memory before computing frequencies and building the tree.  \n",
    "For large datasets, this is inefficient and may exceed memory limits.\n",
    "\n",
    "**Improvement:** Use a *streaming approach* — read data in small chunks and update frequency counts incrementally.  \n",
    "This allows handling larger files efficiently without exhausting memory.\n",
    "\n",
    "---\n",
    "\n",
    "## ** Binary Output Format: Bits vs ASCII Representation**\n",
    "\n",
    "Encoded data are currently stored as ASCII characters (`'0'` and `'1'`), wasting space since each bit uses an entire byte.\n",
    "\n",
    "**Improvement:** Output should be written as a true *bitstream*.  \n",
    "Python libraries such as `bitarray` or `io.BytesIO` can pack bits into bytes efficiently.  \n",
    "During decoding, bytes can be unpacked bit by bit for traversal, ensuring that compression gains accurately reflect theoretical efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## ** Visualization and Interpretability**\n",
    "\n",
    "ASCII trees are easy to inspect for small examples but become unreadable for larger datasets.\n",
    "\n",
    "**Improvement:** Use visualization tools such as `Graphviz` or `NetworkX` to render trees graphically.  \n",
    "Color-coding leaves (symbols) and internal nodes (merged frequencies) would make the construction process more interpretable and pedagogically useful.\n",
    "\n",
    "---\n",
    "\n",
    "## ** Error Handling and Robustness**\n",
    "\n",
    "The current decoder assumes perfect input; a single corrupted bit may cause complete decoding failure.\n",
    "\n",
    "**Improvement:** Introduce input validation and fault tolerance.  \n",
    "Adding checksums or structured exception handling can help detect corrupted files and prevent runtime errors, making the implementation more reliable.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf080f5b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **9. Challenges Faced and Conclusion**\n",
    "\n",
    "---\n",
    "\n",
    "## **Challenges Faced**\n",
    "\n",
    "1. One of the main challenges we faced was setting up the programming environment. Installing Python, configuring the right libraries, and getting Jupyter and Git Bash to work properly took much longer than expected.\n",
    "\n",
    "2. Even though we understood the Huffman algorithm conceptually, turning it into working code was far from straightforward. It took time to make the computer do exactly what we wanted — especially while handling text files, file paths, and encoding issues. Debugging small mistakes like queue ordering or missing cases often took hours.\n",
    "\n",
    "<!-- 3. What surprised us most was how much effort it takes to correctly implement something that seems so basic on paper. Huffman’s algorithm is a simple algorithm, but even small modifications, misunderstandings, or blind spots make it tricky to get right in practice. -->\n",
    "\n",
    "In the end, the experience made us appreciate how theory and implementation are two very different skills — and how much precision and patience real-world coding actually requires.\n",
    "\n",
    "---\n",
    "\n",
    "## **Improvements**\n",
    "\n",
    "Generalizations and future enhancements include:\n",
    "\n",
    "- Implementing binary-level compression to measure true disk savings.  \n",
    "- Developing adaptive Huffman and arithmetic coding variants.  \n",
    "- Applying Huffman coding to multimedia (images, audio).  \n",
    "- Creating a graphical interface to visualize the encoding process.  \n",
    "- Integrating entropy-based analysis to compare performance with the Shannon limit.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "This project successfully implemented and analyzed **Huffman Encoding**, demonstrating its role as a classic example of a **greedy optimization algorithm**.  \n",
    "The results verified that Huffman’s approach minimizes the expected number of bits per symbol while preserving exact reconstructability.  \n",
    "\n",
    "The algorithm was found to be:\n",
    "\n",
    "- **Correct:** Decoding perfectly reconstructs the original text.  \n",
    "- **Efficient:** Exhibits near-linear runtime for practical datasets.  \n",
    "- **Optimal:** Achieves minimal weighted path length among all prefix-free codes.  \n",
    "\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f34c85",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# **Appendix**\n",
    "\n",
    "---\n",
    "\n",
    "## **A. Complete Python Code (Final Submission Version)**\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## **B. How to Use the Code**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
